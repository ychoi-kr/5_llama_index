{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf_XcjXD89hw"
   },
   "source": [
    "# LlamaIndex의 사전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 실습에는 llama-index 0.6.15 이상을 사용해야 합니다.\n",
    "(https://github.com/ychoi-kr/5_llama_index/issues/1 참조) - 옮긴이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mkPbwp6-ploX",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (0.7.11.post1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.5.13)\n",
      "Requirement already satisfied: langchain>=0.0.218 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.0.238)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.15 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (2.0.19)\n",
      "Requirement already satisfied: numpy in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (1.25.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (8.2.2)\n",
      "Requirement already satisfied: openai>=0.26.4 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.27.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (2.0.3)\n",
      "Requirement already satisfied: urllib3<2 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (1.26.16)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (2023.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (4.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from llama-index) (1.5.6)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (3.8.5)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (0.0.12)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from langchain>=0.0.218->llama-index) (2.29.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from dataclasses-json->llama-index) (3.20.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from openai>=0.26.4->llama-index) (4.65.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from sqlalchemy>=2.0.15->llama-index) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from beautifulsoup4->llama-index) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from tiktoken->llama-index) (2023.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (1.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.218->llama-index) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.218->llama-index) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\yong\\anaconda3\\envs\\llamaindex\\lib\\site-packages (from tqdm->openai>=0.26.4->llama-index) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# 패키지 설치\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_Gf8Rfw2MiG_"
   },
   "outputs": [],
   "source": [
    "# 환경 변수 준비\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<OpenAI_API의_API_키>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g9Nyr59eqRUh"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# 로그 레벨 설정\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi1fnUzvgf_c"
   },
   "source": [
    "# 웹페이지에 대한 질문과 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0PZubJiMgvDu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://openai.com:443 \"GET /blog/planning-for-agi-and-beyond HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "# 문서 불러오기\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=[\"https://openai.com/blog/planning-for-agi-and-beyond\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QM94qV6LgvNW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Planning for AGI and beyond\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CloseSe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: believe that democratized access will also lead...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: At some point, it may be important to get indep...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"URL: https://openai.com/blog/planning-for-agi-and-beyond  Planning for AGI and beyond             CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALL\\\\u00b7E 2Customer storiesSafety standardsPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Planning for AGI and beyondOur mission is to ensure that artificial general intelligence\\\\u2014AI systems that are generally smarter than humans\\\\u2014benefits all of\\\\u00a0humanity.Illustration:\\\\u00a0Justin Jay Wang \\\\u00d7 DALL\\\\u00b7EFebruary 24, 2023AuthorsSam AltmanSafety & AlignmentOur mission is to ensure that artificial general intelligence\\\\u2014AI systems that are generally smarter than humans\\\\u2014benefits all of\\\\u00a0humanity.If AGI is successfully created, this technology could help us elevate humanity by increasing abundance, turbocharging the global economy, and aiding in the discovery of new scientific knowledge that changes the limits of\\\\u00a0possibility.AGI has the potential to give everyone incredible new capabilities; we can imagine a world where all of us have access to help with almost any cognitive task, providing a great force multiplier for human ingenuity and\\\\u00a0creativity.On the other hand, AGI would also come with serious risk of misuse, drastic accidents, and societal disruption. Because the upside of AGI is so great, we do not believe it is possible or desirable for society to stop its development forever; instead, society and the developers of AGI have to figure out how to get it right.[^gifts]Although we cannot predict exactly what will happen, and of course our current progress could hit a wall, we can articulate the principles we care about\\\\u00a0most:We want AGI to empower humanity to maximally flourish in the universe. We don\\\\u2019t expect the future to be an unqualified utopia, but we want to maximize the good and minimize the bad, and for AGI to be an amplifier of\\\\u00a0humanity.We want the benefits of, access to, and governance of AGI to be widely and fairly\\\\u00a0shared.We want to successfully navigate massive risks. In confronting these risks, we acknowledge that what seems right in theory often plays out more strangely than expected in practice. We believe we have to continuously learn and adapt by deploying less powerful versions of the technology in order to minimize \\\\u201cone shot to get it right\\\\u201d\\\\u00a0scenarios.The short termThere are several things we think are important to do now to prepare for\\\\u00a0AGI.First, as we create successively more powerful systems, we want to deploy them and gain experience with operating them in the real world. We believe this is the best way to carefully steward AGI into existence\\\\u2014a gradual transition to a world with AGI is better than a sudden one. We expect powerful AI to make the rate of progress in the world much faster, and we think it\\\\u2019s better to adjust to this\\\\u00a0incrementally.A gradual transition gives people, policymakers, and institutions time to understand what\\\\u2019s happening, personally experience the benefits and downsides of these systems, adapt our economy, and to put regulation in place. It also allows for society and AI to co-evolve, and for people collectively to figure out what they want while the stakes are relatively\\\\u00a0low.We currently believe the best way to successfully navigate AI deployment challenges is with a tight feedback loop of rapid learning and careful iteration. Society will face major questions about what AI systems are allowed to do, how to combat bias, how to deal with job displacement, and more. The optimal decisions will depend on the path the technology takes, and like any new field, most expert predictions have been wrong so far. This makes planning in a vacuum very difficult.[^planning]Generally speaking, we think more usage of AI in the world will lead to good, and want to promote it (by putting models in our API, open-sourcing them, etc.). We believe that democratized access will also lead to more and better research, decentralized\", \"URL: https://openai.com/blog/planning-for-agi-and-beyond  believe that democratized access will also lead to more and better research, decentralized power, more benefits, and a broader set of people contributing new\\\\u00a0ideas.As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like. Some people in the AI field think the risks of AGI (and successor systems) are fictitious; we would be delighted if they turn out to be right, but we are going to operate as if these risks are\\\\u00a0existential.At some point, the balance between the upsides and downsides of deployments (such as empowering malicious actors, creating social and economic disruptions, and accelerating an unsafe race) could shift, in which case we would significantly change our plans around continuous\\\\u00a0deployment.As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models.Second, we are working towards creating increasingly aligned and steerable models. Our shift from models like the first version of GPT-3 to\\\\u00a0InstructGPT\\\\u00a0and\\\\u00a0ChatGPT\\\\u00a0is an early example of\\\\u00a0this.In particular, we think it\\\\u2019s important that society agree on extremely wide bounds of how AI can be used, but that within those bounds, individual users have a lot of discretion. Our eventual hope is that the institutions of the world agree on what these wide bounds should be; in the shorter term we plan to run experiments for external input. The institutions of the world will need to be strengthened with additional capabilities and experience to be prepared for complex decisions about\\\\u00a0AGI.The \\\\u201cdefault setting\\\\u201d of our products will likely be quite constrained, but we plan to make it easy for users to change the behavior of the AI they\\\\u2019re using. We believe in empowering individuals to make their own decisions and the inherent power of diversity of\\\\u00a0ideas.We will need to develop\\\\u00a0new alignment techniques\\\\u00a0as our models become more powerful (and tests to understand when our current techniques are failing). Our plan in the shorter term is to\\\\u00a0use AI to help humans evaluate\\\\u00a0the outputs of more complex models and monitor complex systems, and in the longer term to use AI to help us come up with new ideas for better alignment\\\\u00a0techniques.Importantly, we think we often have to make progress on AI safety and capabilities together. It\\\\u2019s a false dichotomy to talk about them separately; they are correlated in many ways. Our best safety work has come from working with our most capable models. That said, it\\\\u2019s important that the ratio of safety progress to capability progress\\\\u00a0increases.Third, we hope for a global conversation about three key questions: how to govern these systems, how to fairly distribute the benefits they generate, and how to fairly share\\\\u00a0access.In addition to these three areas, we have attempted to set up our structure in a way that aligns our incentives with a good outcome. We have\\\\u00a0a clause in our Charter\\\\u00a0about assisting other organizations to advance safety instead of racing with them in late-stage AGI development. We have a cap on the returns our shareholders can earn so that we aren\\\\u2019t incentivized to attempt to capture value without bound and risk deploying something potentially catastrophically dangerous (and of course as a way to share the benefits with society). We have a nonprofit that governs us and lets us operate for the good of humanity (and can override any for-profit interests), including letting us do things like cancel our equity obligations to shareholders if needed for safety and sponsor the world\\\\u2019s most comprehensive UBI\\\\u00a0experiment.We have attempted to set up our structure in a way that aligns our incentives with a good outcome.We think it\\\\u2019s important that efforts like ours submit to independent audits before releasing new systems; we will talk about this in more detail later this year. At some point, it may be important to get independent review before starting to train future systems,\", \"URL: https://openai.com/blog/planning-for-agi-and-beyond  At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models. We think public standards about when an AGI effort should stop a training run, decide a model is safe to release, or pull a model from production use are important. Finally, we think it\\\\u2019s important that major world governments have insight about training runs above a certain\\\\u00a0scale.The long termWe believe that the future of humanity should be determined by humanity, and that it\\\\u2019s important to share information about progress with the public. There should be great scrutiny of all efforts attempting to build AGI and public consultation for major\\\\u00a0decisions.The first AGI will be just a point along the continuum of intelligence. We think it\\\\u2019s likely that progress will continue from there, possibly sustaining the rate of progress we\\\\u2019ve seen over the past decade for a long period of time. If this is true, the world could become extremely different from how it is today, and the risks could be extraordinary. A misaligned superintelligent AGI could cause grievous harm to the world; an autocratic regime with a decisive superintelligence lead could do that\\\\u00a0too.AI that can accelerate science is a special case worth thinking about, and perhaps more impactful than everything else. It\\\\u2019s possible that AGI capable enough to accelerate its own progress could cause major changes to happen surprisingly quickly (and even if the transition starts slowly, we expect it to happen pretty quickly in the final stages). We think a slower takeoff is easier to make safe, and coordination among AGI efforts to slow down at critical junctures will likely be important (even in a world where we don\\\\u2019t need to do this to solve technical alignment problems, slowing down may be important to give society enough time to\\\\u00a0adapt).Successfully transitioning to a world with superintelligence is perhaps the most important\\\\u2014and hopeful, and scary\\\\u2014project in human history. Success is far from guaranteed, and the stakes (boundless downside and boundless upside) will hopefully unite all of\\\\u00a0us.We can imagine a world in which humanity flourishes to a degree that is probably impossible for any of us to fully visualize yet. We hope to contribute to the world an AGI aligned with such\\\\u00a0flourishing.AuthorsSam AltmanView all articlesAcknowledgmentsThanks to Brian Chesky, Paul Christiano, Jack Clark, Holden Karnofsky, Tasha McCauley, Nate Soares, Kevin Scott, Brad Smith, Helen Toner, Allan Dafoe, and the OpenAI team for reviewing drafts of\\\\u00a0this.ResearchOverviewIndexProductOverviewChatGPTGPT-4DALL\\\\u00b7E 2Customer storiesSafety standardsPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI \\\\u00a9 2015\\\\u200a\\\\u2013\\\\u200a2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=131 request_id=fd48019bc0248f68b3961af165f2d8e2 response_code=200\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "# 인덱스 생성\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mFu_QIR4aPFp"
   },
   "outputs": [],
   "source": [
    "# 쿼리 엔진 만들기\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QmLLuYFZgvSk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"\\\\uc774 \\\\uc6f9\\\\ud398\\\\uc774\\\\uc9c0\\\\uc5d0\\\\uc11c \\\\uc804\\\\ud558\\\\uace0 \\\\uc2f6\\\\uc740 \\\\ub9d0\\\\uc740 \\\\ubb34\\\\uc5c7\\\\uc778\\\\uac00\\\\uc694? \\\\ud55c\\\\uad6d\\\\uc5b4\\\\ub85c \\\\ub300\\\\ub2f5\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=58 request_id=8f442353b292217de697253c5ed02a85 response_code=200\n",
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 5b483ea9-52ff-4f7d-af53-110c23eb1a2a] [Similarity score:             0.69597] Planning for AGI and beyond\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CloseSearch Submit Skip to main contentSite NavigationRes...\n",
      "> [Node ba4220e1-69af-4562-8ebd-9a7d49113dc3] [Similarity score:             0.693934] believe that democratized access will also lead to more and better research, decentralized power,...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "DEBUG:openai:api_version=None data='{\"prompt\": \"Context information is below.\\\\n---------------------\\\\nURL: https://openai.com/blog/planning-for-agi-and-beyond\\\\n\\\\nPlanning for AGI and beyond\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALL\\\\u00b7E 2Customer storiesSafety standardsPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Planning for AGI and beyondOur mission is to ensure that artificial general intelligence\\\\u2014AI systems that are generally smarter than humans\\\\u2014benefits all of\\\\u00a0humanity.Illustration:\\\\u00a0Justin Jay Wang \\\\u00d7 DALL\\\\u00b7EFebruary 24, 2023AuthorsSam AltmanSafety & AlignmentOur mission is to ensure that artificial general intelligence\\\\u2014AI systems that are generally smarter than humans\\\\u2014benefits all of\\\\u00a0humanity.If AGI is successfully created, this technology could help us elevate humanity by increasing abundance, turbocharging the global economy, and aiding in the discovery of new scientific knowledge that changes the limits of\\\\u00a0possibility.AGI has the potential to give everyone incredible new capabilities; we can imagine a world where all of us have access to help with almost any cognitive task, providing a great force multiplier for human ingenuity and\\\\u00a0creativity.On the other hand, AGI would also come with serious risk of misuse, drastic accidents, and societal disruption. Because the upside of AGI is so great, we do not believe it is possible or desirable for society to stop its development forever; instead, society and the developers of AGI have to figure out how to get it right.[^gifts]Although we cannot predict exactly what will happen, and of course our current progress could hit a wall, we can articulate the principles we care about\\\\u00a0most:We want AGI to empower humanity to maximally flourish in the universe. We don\\\\u2019t expect the future to be an unqualified utopia, but we want to maximize the good and minimize the bad, and for AGI to be an amplifier of\\\\u00a0humanity.We want the benefits of, access to, and governance of AGI to be widely and fairly\\\\u00a0shared.We want to successfully navigate massive risks. In confronting these risks, we acknowledge that what seems right in theory often plays out more strangely than expected in practice. We believe we have to continuously learn and adapt by deploying less powerful versions of the technology in order to minimize \\\\u201cone shot to get it right\\\\u201d\\\\u00a0scenarios.The short termThere are several things we think are important to do now to prepare for\\\\u00a0AGI.First, as we create successively more powerful systems, we want to deploy them and gain experience with operating them in the real world. We believe this is the best way to carefully steward AGI into existence\\\\u2014a gradual transition to a world with AGI is better than a sudden one. We expect powerful AI to make the rate of progress in the world much faster, and we think it\\\\u2019s better to adjust to this\\\\u00a0incrementally.A gradual transition gives people, policymakers, and institutions time to understand what\\\\u2019s happening, personally experience the benefits and downsides of these systems, adapt our economy, and to put regulation in place. It also allows for society and AI to co-evolve, and for people collectively to figure out what they want while the stakes are relatively\\\\u00a0low.We currently believe the best way to successfully navigate AI deployment challenges is with a tight feedback loop of rapid learning and careful iteration. Society will face major questions about what AI systems are allowed to do, how to combat bias, how to deal with job displacement, and more. The optimal decisions will depend on the path the technology takes, and like any new field, most expert predictions have been wrong so far. This makes planning in a vacuum very difficult.[^planning]Generally speaking, we think more usage of AI in the world will lead to good, and want to promote it (by putting models in our API, open-sourcing them, etc.). We believe that democratized access will also lead to more and better research, decentralized\\\\n\\\\nURL: https://openai.com/blog/planning-for-agi-and-beyond\\\\n\\\\nbelieve that democratized access will also lead to more and better research, decentralized power, more benefits, and a broader set of people contributing new\\\\u00a0ideas.As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like. Some people in the AI field think the risks of AGI (and successor systems) are fictitious; we would be delighted if they turn out to be right, but we are going to operate as if these risks are\\\\u00a0existential.At some point, the balance between the upsides and downsides of deployments (such as empowering malicious actors, creating social and economic disruptions, and accelerating an unsafe race) could shift, in which case we would significantly change our plans around continuous\\\\u00a0deployment.As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models.Second, we are working towards creating increasingly aligned and steerable models. Our shift from models like the first version of GPT-3 to\\\\u00a0InstructGPT\\\\u00a0and\\\\u00a0ChatGPT\\\\u00a0is an early example of\\\\u00a0this.In particular, we think it\\\\u2019s important that society agree on extremely wide bounds of how AI can be used, but that within those bounds, individual users have a lot of discretion. Our eventual hope is that the institutions of the world agree on what these wide bounds should be; in the shorter term we plan to run experiments for external input. The institutions of the world will need to be strengthened with additional capabilities and experience to be prepared for complex decisions about\\\\u00a0AGI.The \\\\u201cdefault setting\\\\u201d of our products will likely be quite constrained, but we plan to make it easy for users to change the behavior of the AI they\\\\u2019re using. We believe in empowering individuals to make their own decisions and the inherent power of diversity of\\\\u00a0ideas.We will need to develop\\\\u00a0new alignment techniques\\\\u00a0as our models become more powerful (and tests to understand when our current techniques are failing). Our plan in the shorter term is to\\\\u00a0use AI to help humans evaluate\\\\u00a0the outputs of more complex models and monitor complex systems, and in the longer term to use AI to help us come up with new ideas for better alignment\\\\u00a0techniques.Importantly, we think we often have to make progress on AI safety and capabilities together. It\\\\u2019s a false dichotomy to talk about them separately; they are correlated in many ways. Our best safety work has come from working with our most capable models. That said, it\\\\u2019s important that the ratio of safety progress to capability progress\\\\u00a0increases.Third, we hope for a global conversation about three key questions: how to govern these systems, how to fairly distribute the benefits they generate, and how to fairly share\\\\u00a0access.In addition to these three areas, we have attempted to set up our structure in a way that aligns our incentives with a good outcome. We have\\\\u00a0a clause in our Charter\\\\u00a0about assisting other organizations to advance safety instead of racing with them in late-stage AGI development. We have a cap on the returns our shareholders can earn so that we aren\\\\u2019t incentivized to attempt to capture value without bound and risk deploying something potentially catastrophically dangerous (and of course as a way to share the benefits with society). We have a nonprofit that governs us and lets us operate for the good of humanity (and can override any for-profit interests), including letting us do things like cancel our equity obligations to shareholders if needed for safety and sponsor the world\\\\u2019s most comprehensive UBI\\\\u00a0experiment.We have attempted to set up our structure in a way that aligns our incentives with a good outcome.We think it\\\\u2019s important that efforts like ours submit to independent audits before releasing new systems; we will talk about this in more detail later this year. At some point, it may be important to get independent review before starting to train future systems,\\\\n---------------------\\\\nGiven the context information and not prior knowledge, answer the question: \\\\uc774 \\\\uc6f9\\\\ud398\\\\uc774\\\\uc9c0\\\\uc5d0\\\\uc11c \\\\uc804\\\\ud558\\\\uace0 \\\\uc2f6\\\\uc740 \\\\ub9d0\\\\uc740 \\\\ubb34\\\\uc5c7\\\\uc778\\\\uac00\\\\uc694? \\\\ud55c\\\\uad6d\\\\uc5b4\\\\ub85c \\\\ub300\\\\ub2f5\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\\\n\", \"stream\": false, \"model\": \"text-davinci-003\", \"temperature\": 0.0, \"max_tokens\": 2263}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=16895 request_id=9befe3513a2cd91aa4faee66c3bbe94f response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:\n",
      "OpenAI의 미션은 인간보다 일반적으로 더 똑똑한 인공 일반 지능(AGI)이 인류 전체에게 도움이 되도록 보장하는 것입니다. 우리는 AGI가 인류를 가장 잘 행복하게 만들 수 있도록 권력과 접근, 그리고 관리를 폭넓게 공정하게 나누는 것을 원합니다. 우리는 또한 엄청난 위험을 성공적으로 가로지를 수 있는 방법을 찾고자 합니다. 우리는 이러한 위험이 실제로 존재한다고 생각하고 있습니다.\n",
      "\n",
      "OpenAI의 미션은 인간보다 일반적으로 더 똑똑한 인공 일반 지능(AGI)이 인류 전체에게 도움이 되도록 보장하는 것입니다. 우리는 AGI가 인류를 가장 잘 행복하게 만들 수 있도록 권력과 접근, 그리고 관리를 폭넓게 공정하게 나누는 것을 원합니다. 우리는 또한 엄청난 위험을 성공적으로 가로지를 수 있는 방법을 찾고자 합니다. 우리는 이러한 위험이 실제로 존재한다고 생각하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 질의응답\n",
    "print(query_engine.query(\"이 웹페이지에서 전하고 싶은 말은 무엇인가요? 한국어로 대답해 주세요.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wL-FfZDW4vK"
   },
   "source": [
    "# 유튜브 동영상에 대한 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6K-q2N86mwOS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.youtube.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.youtube.com:443 \"GET /watch?v=oc6RV5c1yd0 HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.youtube.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.youtube.com:443 \"GET /api/timedtext?v=oc6RV5c1yd0&caps=asr&opi=112496729&xoaf=5&hl=ko&ip=0.0.0.0&ipbits=0&expire=1689958538&sparams=ip,ipbits,expire,v,caps,opi,xoaf&signature=7ABB50C16B067628F3DDDD84A20292DD290C83BC.BC15075603D88C35E126427D78AFC05CCF2F1475&key=yt8&lang=en HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "# 문서 불러오기\n",
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=[\"https://www.youtube.com/watch?v=oc6RV5c1yd0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WsBr4NJx50By"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: GPT-4 is the latest AI system from OpenAI.\n",
      "The ...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"video_id: oc6RV5c1yd0  GPT-4 is the latest AI system from OpenAI. The lab that created Dall-E. And ChatGPT. GPT-4 is a breakthrough in problem solving capabilities. For example, you can ask it how you would clean the inside of a tank filled with piranhas. And it\\'ll give you something useful. It can also read, analyze, or generate up to 25,000 words of text. It can write code in all major programming languages. And it understands images as input, and can reason with them in sophisticated ways. Most importantly, after we created GPT-4, we spent months making it safer and more aligned with how you want to use it. The methods we\\'ve developed to continuously improve GPT-4 will help us as we work towards AI systems that will empower us all.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=161 request_id=bc78334e8f218f074a193b520fca9cfb response_code=200\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "# 인덱스 생성\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P3u7gtP9ar2k"
   },
   "outputs": [],
   "source": [
    "# 쿼리 엔진 생성\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fVtBdcVHQHub"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"\\\\uc774 \\\\ub3d9\\\\uc601\\\\uc0c1\\\\uc5d0\\\\uc11c \\\\uc804\\\\ud558\\\\uace0 \\\\uc2f6\\\\uc740 \\\\ub9d0\\\\uc740 \\\\ubb34\\\\uc5c7\\\\uc778\\\\uac00\\\\uc694? \\\\ud55c\\\\uad6d\\\\uc5b4\\\\ub85c \\\\ub300\\\\ub2f5\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=36 request_id=b66d4efab151cc31bbb14450cc0f3313 response_code=200\n",
      "DEBUG:llama_index.indices.utils:> Top 1 nodes:\n",
      "> [Node 22501f01-a720-4696-ba7b-6cd25c09a6c1] [Similarity score:             0.723358] GPT-4 is the latest AI system from OpenAI.\n",
      "The lab that created Dall-E.\n",
      "And ChatGPT.\n",
      "GPT-4 is a b...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "DEBUG:openai:api_version=None data='{\"prompt\": \"Context information is below.\\\\n---------------------\\\\nvideo_id: oc6RV5c1yd0\\\\n\\\\nGPT-4 is the latest AI system from OpenAI.\\\\nThe lab that created Dall-E.\\\\nAnd ChatGPT.\\\\nGPT-4 is a breakthrough in\\\\nproblem solving capabilities.\\\\nFor example, you can ask it how\\\\nyou would clean the inside of\\\\na tank filled with piranhas.\\\\nAnd it\\'ll give you something useful.\\\\nIt can also read, analyze, or\\\\ngenerate up to 25,000 words of text.\\\\nIt can write code in all\\\\nmajor programming languages.\\\\nAnd it understands images as input,\\\\nand can reason with them\\\\nin sophisticated ways.\\\\nMost importantly, after we created GPT-4,\\\\nwe spent months making it safer and more\\\\naligned with how you want to use it.\\\\nThe methods we\\'ve developed to\\\\ncontinuously improve GPT-4 will\\\\nhelp us as we work towards AI\\\\nsystems that will empower us all.\\\\n---------------------\\\\nGiven the context information and not prior knowledge, answer the question: \\\\uc774 \\\\ub3d9\\\\uc601\\\\uc0c1\\\\uc5d0\\\\uc11c \\\\uc804\\\\ud558\\\\uace0 \\\\uc2f6\\\\uc740 \\\\ub9d0\\\\uc740 \\\\ubb34\\\\uc5c7\\\\uc778\\\\uac00\\\\uc694? \\\\ud55c\\\\uad6d\\\\uc5b4\\\\ub85c \\\\ub300\\\\ub2f5\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\\\n\", \"stream\": false, \"model\": \"text-davinci-003\", \"temperature\": 0.0, \"max_tokens\": 3781}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=3053 request_id=a99c0c01ff929c877078eab5a2636f36 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:\n",
      "이 동영상에서 전하고 싶은 말은 GPT-4가 우리가 원하는 방식으로 사용할 수 있도록 안전하고 정확하게 개발되었다는 것입니다.\n",
      "\n",
      "이 동영상에서 전하고 싶은 말은 GPT-4가 우리가 원하는 방식으로 사용할 수 있도록 안전하고 정확하게 개발되었다는 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 질의응답\n",
    "print(query_engine.query(\"이 동영상에서 전하고 싶은 말은 무엇인가요? 한국어로 대답해 주세요.\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO5vPiDwtbBCC8IJtWVmpB9",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
